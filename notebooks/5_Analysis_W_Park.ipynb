{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# San Francisco Parking Exploratory Data Analysis\n",
    "this notebook is for the in depth analyis of the San Francisco Parking Ticket data. The main goal of this will be to answer four questions. \n",
    "\n",
    "1. Can we identify streets that are less likely for enforcement officer to travel through, when looking for residential overtime tickets?\n",
    "\n",
    "2. How long can we expect to park before we have to move our car?\n",
    "\n",
    "3. Which hour of street cleaning is most effective at getting people ticketed?\n",
    "\n",
    "4. Does less frequent street cleaning get mroe tickets per sweep?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import datetime as dt\n",
    "import time\n",
    "from scipy import stats\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import sqlite3\n",
    "import math\n",
    "import seaborn as sns\n",
    "import statsmodels.formula as sm\n",
    "import scipy \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_query(querystring):\n",
    "    resultdf = pd.read_sql(sql= querystring, con = conn)\n",
    "    \n",
    "    return resultdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_folder = '/home/colin/Desktop/SF_Parking/data/raw/'\n",
    "proc_folder = '/home/colin/Desktop/SF_Parking/data/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('SF_Parking.db')\n",
    "c = conn.cursor()\n",
    "tables = result_query(\"SELECT name FROM sqlite_master where type = 'table'\")\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "Let's test our initial hypothesis, but include street parking availability now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'Select distinct t3.lineid, t3.streetname, nhood, distance, total_ea, vvol_carea, vvol_trkea, vvol_busea, speed_ea, oneway, count(*) total_tickets, park_supply from ticket_data t1 join address_data t2 on t1.address = t2.address  join street_volume_data t3 on t2.lineid = t3.lineid  Where ViolationDesc = 'RES/OT'  group by t3.lineid': no such table: ticket_data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m                 \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: ticket_data",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c5082f02bfe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Lets categorize addresses by our street volume\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m streets = result_query(\"Select distinct t3.lineid, t3.streetname, nhood, distance, total_ea, vvol_carea, vvol_trkea, vvol_busea, speed_ea, oneway, count(*) total_tickets, park_supply \"\n\u001b[0m\u001b[1;32m      3\u001b[0m                        \u001b[0;34m'from ticket_data t1 join address_data t2 on t1.address = t2.address '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                        \u001b[0;34m' join street_volume_data t3 on t2.lineid = t3.lineid '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                        \" Where ViolationDesc = 'RES/OT'  group by t3.lineid\")\n",
      "\u001b[0;32m<ipython-input-2-beaaf91b74d5>\u001b[0m in \u001b[0;36mresult_query\u001b[0;34m(querystring)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mresult_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquerystring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mresultdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mquerystring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresultdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             chunksize=chunksize)\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1388\u001b[0m             ex = DatabaseError(\n\u001b[1;32m   1389\u001b[0m                 \"Execution failed on sql '%s': %s\" % (args[0], exc))\n\u001b[0;32m-> 1390\u001b[0;31m             \u001b[0mraise_with_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/compat/__init__.py\u001b[0m in \u001b[0;36mraise_with_traceback\u001b[0;34m(exc, traceback)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEllipsis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;31m# this version of raise is a syntax error in Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1376\u001b[0m                 \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m                 \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql 'Select distinct t3.lineid, t3.streetname, nhood, distance, total_ea, vvol_carea, vvol_trkea, vvol_busea, speed_ea, oneway, count(*) total_tickets, park_supply from ticket_data t1 join address_data t2 on t1.address = t2.address  join street_volume_data t3 on t2.lineid = t3.lineid  Where ViolationDesc = 'RES/OT'  group by t3.lineid': no such table: ticket_data"
     ]
    }
   ],
   "source": [
    "#Lets categorize addresses by our street volume \n",
    "streets = result_query(\"Select distinct t3.lineid, t3.streetname, nhood, distance, total_ea, vvol_carea, vvol_trkea, vvol_busea, speed_ea, oneway, count(*) total_tickets, park_supply \"\n",
    "                       'from ticket_data t1 join address_data t2 on t1.address = t2.address ' \n",
    "                       ' join street_volume_data t3 on t2.lineid = t3.lineid '\n",
    "                       \" Where ViolationDesc = 'RES/OT'  group by t3.lineid\")\n",
    "c.execute('Select Max(TickIssueDate), Min(TickIssueDate) from ticket_data')\n",
    "totaldays = c.fetchone()\n",
    "maxdate = time.strptime( totaldays[0], '%Y-%m-%d %H:%M:%S')\n",
    "mindate = time.strptime( totaldays[1], '%Y-%m-%d %H:%M:%S')\n",
    "totaldays = (time.mktime(maxdate) - time.mktime(mindate)) / (60*60*24)\n",
    "totalyears = totaldays / 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalyears"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a few new columns and fill in some bad records. First Let's fix the parking supply column. If anyone got a residential overtime spot, there was clearly some parking available. Let's use the neighborhood average spots per mile to fill this in where its null or 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets['parkpermile'] = streets['park_supply'] / streets['distance']\n",
    "streets_mean = streets[streets.park_supply > 0 ].groupby(by = ['nhood'], as_index = False)['parkpermile'].mean()\n",
    "streets_1 = streets[streets.park_supply > 0 ]\n",
    "streets_2 = streets[(streets.park_supply== 0) | (pd.isnull(streets.park_supply)) ]\n",
    "streets_2 = streets_2.merge(streets_mean, left_on = 'nhood', right_on = 'nhood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(streets_1.shape)\n",
    "print(streets_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets_2['park_supply'] = streets_2['parkpermile_y'] * streets_2['distance']\n",
    "streets_2.rename(columns = {'parkpermile_y':'parkpermile'}, inplace = True)\n",
    "streets_2.drop(columns =['parkpermile_x'], inplace = True)\n",
    "streets = streets_1.append(streets_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to avoid any problems log transforming, we'll add 1 to all street volume\n",
    "streets['total_ea'] = streets['total_ea'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "fig, axplots = plt.subplots(2,2, figsize = (15, 15))\n",
    "\n",
    "axplots[0,0].hist(streets.distance, bins = 'auto', normed = True)\n",
    "axplots[0,0].set_xlabel('distance')\n",
    "\n",
    "axplots[0,1].hist(streets.total_ea, bins = 'auto')\n",
    "axplots[0,1].set_xlabel('Total Street Volume')\n",
    "axplots[0,1].set_ylim(0,300)\n",
    "\n",
    "axplots[1,0].hist(streets.total_tickets, bins = 'auto' )\n",
    "axplots[1,0].set_xlabel('Total Tickets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Log Transform a few to see if they appear a little more normal\n",
    "from scipy.stats import norm\n",
    "\n",
    "fig, axplots = plt.subplots(2, figsize = (10, 10))\n",
    "log_volume = np.log(streets['total_ea'])\n",
    "log_tickets = np.log(streets['total_tickets'])\n",
    "vol_mean = log_volume.mean()\n",
    "tick_mean = log_tickets.mean()\n",
    "vol_std = log_volume.std()\n",
    "tick_std = log_tickets.std()\n",
    "vol_normals = norm(loc = vol_mean, scale = vol_std)\n",
    "vol = np.linspace(vol_normals.ppf(0.01), \n",
    "                        vol_normals.ppf(0.99), 100)\n",
    "\n",
    "tick_normals = norm(loc = tick_mean, scale = tick_std)\n",
    "ticks = np.linspace(tick_normals.ppf(0.01), \n",
    "                        vol_normals.ppf(0.99), \n",
    "                        100)\n",
    "\n",
    "\n",
    "axplots[0].hist(log_volume, bins = 'auto', normed = True )\n",
    "axplots[0].set_xlabel('Total Street Volume(log)')\n",
    "axplots[0].plot(vol, vol_normals.pdf(vol))\n",
    "\n",
    "axplots[1].hist(np.log(streets.total_tickets), bins = 'auto', normed = True )\n",
    "axplots[1].set_xlabel('Total Tickets(log)')\n",
    "axplots[1].plot(ticks, tick_normals.pdf(ticks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets look at the percentiles of streetvolume\n",
    "plt.figure(figsize = (10,6))\n",
    "counts, bin_edges = np.histogram(streets['total_ea'], bins=30, normed=True)\n",
    "cdf = np.cumsum (counts)\n",
    "plt.plot(bin_edges[1:], cdf/cdf[-1])\n",
    "plt.xlabel('Total Street Volume')\n",
    "plt.ylabel('Cumulative %')\n",
    "plt.title('Cumulative Distribution of Street Volume')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in [25, 50, 75, 100]:\n",
    "    print (\"{}%% percentile: {}\".format (q, np.percentile(streets['total_ea'], q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets scatter volume against total tickets\n",
    "\n",
    "plt.figure(figsize = (15, 15))\n",
    "plt.scatter(x = np.log(streets['total_ea']), y = streets['total_tickets'])\n",
    "plt.xlabel('total volume(log)')\n",
    "plt.ylabel('total_tickets')\n",
    "plt.title('Scatter Plot of Street Volume vs. Total Tickets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do the same, but create a new measure, tickets per 100 parking spaces \n",
    "streets['tickperspot'] = streets['total_tickets'] / (streets['park_supply'] / 100) / totalyears\n",
    "plt.figure(figsize = (15, 15))\n",
    "plt.scatter(x = np.log(streets['total_ea']), y = streets['tickperspot'], cmap = 'RdYlBu', c = streets['vvol_busea'])\n",
    "plt.title('Scatter Plot of Total Street Volume vs. Total Tickers per 100 spots per Year, Colored by Bus Volume')\n",
    "plt.ylabel('Tickets per 100 spots per year')\n",
    "plt.xlabel('Total Street Volume')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the higher volume streets get any bus volume, and they all seem to be lower tickets per spot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of spots per street\n",
    "streets['park_supply'].hist(bins = 'auto')\n",
    "plt.title('Total Spots per Street Histogram')\n",
    "plt.xlabel('Total Spots')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's map all of the streets we've identified \n",
    "streetvolume = gpd.read_file(proc_folder + './final_streets/SF_Street_Data.shp')\n",
    "\n",
    "streetvolume = streetvolume.to_crs(epsg = 4326)\n",
    "times = ['am', 'pm', 'ev', 'ea']\n",
    "for time in times:\n",
    "    streetvolume['totalinv_' + time]  = streetvolume['total_'+time].apply(lambda x: np.log(1/(x+.5)))\n",
    "    \n",
    "df = streetvolume.merge(streets, left_on = 'lineid', right_on = 'lineid')\n",
    "    \n",
    "df.plot(figsize = (20,20), cmap = 'RdYlGn', column = 'tickperspot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets['total_tickets'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets split into two populations based on their street volume, we will then create a weighted total average of tickets per 100 spots\n",
    "df_lowvol = streets[streets.total_ea <=  np.percentile(streets['total_ea'], 50)]\n",
    "df_highvol = streets[streets.total_ea > np.percentile(streets['total_ea'], 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lowvol['weight'] = df_lowvol['park_supply'] / df_lowvol['park_supply'].sum()\n",
    "(df_lowvol['weight'] * df_lowvol['tickperspot']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_highvol['weight'] = df_highvol['park_supply'] / df_highvol['park_supply'].sum()\n",
    "(df_highvol['weight'] * df_highvol['tickperspot']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are extremely similar, and highger street volume appears to correlate with mroe tickets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's split into smaller groups\n",
    "df_1 = streets[streets.total_ea <=  np.percentile(streets['total_ea'], 25)]\n",
    "df_2 = streets[(streets.total_ea <= np.percentile(streets['total_ea'], 50)) & (streets.total_ea >  np.percentile(streets['total_ea'], 25))]\n",
    "df_3 = streets[(streets.total_ea <= np.percentile(streets['total_ea'], 75)) & (streets.total_ea >  np.percentile(streets['total_ea'], 50))]\n",
    "df_4 = streets[streets.total_ea > np.percentile(streets['total_ea'], 75)]\n",
    "\n",
    "df_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it may be wise to use the central limit theorem, because you will not have the availablility of streets at your fingertips when finding a parking spot. Let's do what we did in the first notebook, but on our new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since so many have the same value, we should probably filter the group on a sorted index so the sample sizes are all the same \n",
    "streets.sort_values(by = 'total_ea', inplace = True)\n",
    "streets.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "means = {}\n",
    "stds = {}\n",
    "for i in np.arange(1,11):\n",
    "    if i == 1:\n",
    "        df = streets[streets.index <=  np.percentile(streets.index, i*10)]\n",
    "    else:\n",
    "        df = streets[(streets.index <=  np.percentile(streets.index, i*10)) & (streets.index >  np.percentile(streets.index, (i-1)*10))]\n",
    "               \n",
    "    sample = []\n",
    "    for j in np.arange(1,1000):\n",
    "        sample.append(df['tickperspot'].sample(n = 20).median())\n",
    "        \n",
    "    sample = np.array(sample)\n",
    "        \n",
    "    means[i] = sample.mean()\n",
    "    stds[i] = sample.std()\n",
    "    normals = norm(loc = means[i], scale = stds[i])\n",
    "    \n",
    "    x = np.linspace(normals.ppf(0.01), \n",
    "                        normals.ppf(0.99), \n",
    "                        100)\n",
    "    labelstr = str(i * 10) + '%'\n",
    "    plt.plot(x, normals.pdf(x), label = labelstr, color =  plt.cm.RdYlGn(i/10))\n",
    "plt.legend( loc = 0)\n",
    "plt.title('Distribution curves of sampled street populations sirted and split by street volume ')\n",
    "plt.xlabel('Tickets per 100 spots per year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, it looks like our lower volume streets get less tickets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "means = {}\n",
    "stds = {}\n",
    "\n",
    "for i in np.arange(1,21):\n",
    "    if i == 1:\n",
    "        df = streets[streets.index <=  np.percentile(streets.index, i*5)]\n",
    "    else:\n",
    "        df = streets[(streets.index <=  np.percentile(streets.index, i*5)) & (streets.index >  np.percentile(streets.index, (i-1)*5))]\n",
    "              \n",
    "    sample = []\n",
    "    for j in np.arange(1,1000):\n",
    "        sample.append(df['tickperspot'].sample(n = 20).median())\n",
    "        \n",
    "    sample = np.array(sample)\n",
    "        \n",
    "    means[i] = sample.mean()\n",
    "    stds[i] = sample.std()\n",
    "    normals = norm(loc = means[i], scale = stds[i])\n",
    "    \n",
    "    x = np.linspace(normals.ppf(0.01), \n",
    "                        normals.ppf(0.99), \n",
    "                        100)\n",
    "    labelstr = 'pop_' + str(i)\n",
    "    plt.plot(x, normals.pdf(x), label = labelstr, color =  plt.cm.RdYlGn(i/20))\n",
    "plt.legend( loc = 0)\n",
    "plt.title('Frequency curves of sampled street populations split by street volume ')\n",
    "plt.xlabel('Tickets per 100 spots per year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can create another model using our new information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "columns = ['vvol_trkea', 'vvol_carea', 'vvol_busea', 'speed_ea', 'parkpermile', 'distance', 'oneway']\n",
    "\n",
    "model = sm.OLS.from_formula('tickperspot ~ '+ '+'.join(columns) , streets)\n",
    "res = model.fit()\n",
    "plt.rc('figure', figsize=(12, 7))\n",
    "plt.text(0.01, 0.05, str(res.summary()), {'fontsize': 10}, fontproperties = 'monospace') \n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, car volume wasn't very statistically significant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets['fitted'] =  res.fittedvalues\n",
    "streets.sort_values(by = 'fitted', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "means = {}\n",
    "stds = {}\n",
    "totalsize = streets.shape[0]\n",
    "for i in np.arange(1,11):\n",
    "    if i == 1:\n",
    "        df = streets[0:int(totalsize * .1)]\n",
    "    else:\n",
    "        df = streets[((i-1)/10 * totalsize).astype(int): (((i)/10) * totalsize).astype(int)]\n",
    "        \n",
    "    sample = []\n",
    "    for j in np.arange(1,1000):\n",
    "        sample.append(df['tickperspot'].sample(n = 20).median())\n",
    "        \n",
    "    sample = np.array(sample)\n",
    "        \n",
    "    means[i] = sample.mean()\n",
    "    stds[i] = sample.std()\n",
    "    normals = norm(loc = means[i], scale = stds[i])\n",
    "    \n",
    "    x = np.linspace(normals.ppf(0.01), \n",
    "                        normals.ppf(0.99), \n",
    "                        100)\n",
    "    labelstr = str(i * 10) + '%'\n",
    "    plt.plot(x, normals.pdf(x), label = labelstr, color =  plt.cm.RdYlGn(1-(i/10)))\n",
    "plt.legend( loc = 0)\n",
    "plt.title('Frequency curves of sampled street populations split by OLS fitted values ')\n",
    "plt.xlabel('Tickets per 100 spots per year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the original model, this one fits much better when using it to decide streets to park at. Our r^2 is also slightly higher, although still not very high. There is a large amount of variability, and a very large sample size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Higher our variables get, the fit seems to be stronger, which would confirm that using these measures may be an accurate way to estimate ticket probability. Let's check out some diagnostic plots to confirm this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = res\n",
    "# fitted \n",
    "model_fitted_y = model_fit.fittedvalues\n",
    "\n",
    "# residuals\n",
    "model_residuals = model_fit.resid\n",
    "\n",
    "# normalized residuals\n",
    "model_norm_residuals = model_fit.get_influence().resid_studentized_internal\n",
    "\n",
    "# absolute squared normalized residuals\n",
    "model_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n",
    "\n",
    "# absolute residuals\n",
    "model_abs_resid = np.abs(model_residuals)\n",
    "\n",
    "# leverage, from statsmodels internals\n",
    "model_leverage = model_fit.get_influence().hat_matrix_diag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "#All four diagnostic plots in one place\n",
    "fig, axarr = plt.subplots(2,2, figsize = (20,20))\n",
    "QQ = ProbPlot(model_norm_residuals)\n",
    "\n",
    "#residuals \n",
    "sns.residplot( model_fitted_y, 'tickperspot', data=streets, \n",
    "                          lowess=True, \n",
    "                          scatter_kws={'alpha': 0.5}, \n",
    "                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8}, ax = axarr[0,0])\n",
    "axarr[0,0].set_xlabel('Fitted')\n",
    "axarr[0,0].set_ylabel('Residual')\n",
    "\n",
    "\n",
    "#Q-Q\n",
    "QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1, ax = axarr[0,1])\n",
    "axarr[0,1].set_xlabel('Theoretical Quantile')\n",
    "axarr[0,1].set_ylabel('Residuals')\n",
    "axarr[0,1].set_xlim(-4,4)\n",
    "\n",
    "\n",
    "\n",
    "#scale - location\n",
    "axarr[1,0].scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5)\n",
    "sns.regplot(model_fitted_y, model_norm_residuals_abs_sqrt, \n",
    "            scatter=False, \n",
    "            ci=False, \n",
    "            lowess=True,\n",
    "            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8}, ax = axarr[1,0])\n",
    "axarr[1,0].set_xlabel('Fitted')\n",
    "axarr[1,0].set_ylabel('Standardized')\n",
    "\n",
    "\n",
    "#Leverage\n",
    "axarr[1,1].scatter(model_leverage, model_norm_residuals, alpha=0.5)\n",
    "sns.regplot(model_leverage, model_norm_residuals, \n",
    "            scatter=False, \n",
    "            ci=False, \n",
    "            lowess=True,\n",
    "            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8}, ax = axarr[1,1])\n",
    "axarr[1,1].set_xlim(0,0.1)\n",
    "axarr[1,1].set_xlabel('Leverage')\n",
    "axarr[1,1].set_ylabel('Standardized Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-Q Plot indicates there may be a non-linear relationship here. Let's try a few other fits. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There seems to be a non-linear relationship. in order to log fit, we need to remove all zeroes, so well add 1 to each category and then doa log fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = streets\n",
    "for column in columns:\n",
    "    df[column] = df[column] + 0.01\n",
    "    \n",
    "formstring = 'tickperspot ~np.log(vvol_trkea)+np.log(vvol_carea)+np.log(vvol_busea)+np.log(speed_ea) + np.log(parkpermile) + oneway'\n",
    "model = sm.OLS.from_formula(formstring , streets)\n",
    "res = model.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Log fit seems to be a little better. \n",
    "model_fit = res\n",
    "# fitted \n",
    "model_fitted_y = model_fit.fittedvalues\n",
    "\n",
    "# residuals\n",
    "model_residuals = model_fit.resid\n",
    "\n",
    "# normalized residuals\n",
    "model_norm_residuals = model_fit.get_influence().resid_studentized_internal\n",
    "\n",
    "# absolute squared normalized residuals\n",
    "model_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n",
    "\n",
    "# absolute residuals\n",
    "model_abs_resid = np.abs(model_residuals)\n",
    "\n",
    "# leverage, from statsmodels internals\n",
    "model_leverage = model_fit.get_influence().hat_matrix_diag\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "#All four diagnostic plots in one place\n",
    "fig, axarr = plt.subplots(2,2, figsize = (20,20))\n",
    "QQ = ProbPlot(model_norm_residuals)\n",
    "\n",
    "#residuals \n",
    "sns.residplot( model_fitted_y, 'tickperspot', data=streets, \n",
    "                          lowess=True, \n",
    "                          scatter_kws={'alpha': 0.5}, \n",
    "                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8}, ax = axarr[0,0])\n",
    "axarr[0,0].set_xlabel('Fitted')\n",
    "axarr[0,0].set_ylabel('Residual')\n",
    "\n",
    "\n",
    "#Q-Q\n",
    "QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1, ax = axarr[0,1])\n",
    "axarr[0,1].set_xlabel('Theoretical Quantile')\n",
    "axarr[0,1].set_ylabel('Residuals')\n",
    "axarr[0,1].set_xlim(-4,4)\n",
    "\n",
    "\n",
    "\n",
    "#scale - location\n",
    "axarr[1,0].scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5)\n",
    "sns.regplot(model_fitted_y, model_norm_residuals_abs_sqrt, \n",
    "            scatter=False, \n",
    "            ci=False, \n",
    "            lowess=True,\n",
    "            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8}, ax = axarr[1,0])\n",
    "axarr[1,0].set_xlabel('Fitted')\n",
    "axarr[1,0].set_ylabel('Standardized')\n",
    "\n",
    "\n",
    "#Leverage\n",
    "axarr[1,1].scatter(model_leverage, model_norm_residuals, alpha=0.5)\n",
    "sns.regplot(model_leverage, model_norm_residuals, \n",
    "            scatter=False, \n",
    "            ci=False, \n",
    "            lowess=True,\n",
    "            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8}, ax = axarr[1,1])\n",
    "axarr[1,1].set_xlim(0,0.1)\n",
    "axarr[1,1].set_xlabel('Leverage')\n",
    "axarr[1,1].set_ylabel('Standardized Residuals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-do our central limit exercise once again. \n",
    "streets['fitted'] =  res.fittedvalues\n",
    "streets.sort_values(by = 'fitted', inplace = True)\n",
    "plt.figure(figsize = (10, 10))\n",
    "means = {}\n",
    "stds = {}\n",
    "df = streets\n",
    "sample = []\n",
    "boxdata = []\n",
    "#create baseline curve\n",
    "for j in np.arange(1,1000):\n",
    "    sample.append(df['tickperspot'].sample(n = 20).median())\n",
    " \n",
    "\n",
    "sample = np.array(sample)\n",
    "\n",
    "means['base'] = sample.mean()\n",
    "stds['base'] = sample.std()\n",
    "normals = norm(loc = means['base'], scale = stds['base'])\n",
    "\n",
    "x = np.linspace(normals.ppf(0.01), \n",
    "                    normals.ppf(0.99), \n",
    "                    100)\n",
    "labelstr = str(i * 10) + '%'\n",
    "plt.plot(x, normals.pdf(x), label = 'Baseline', color = 'black', linestyle = '--')\n",
    "\n",
    "        \n",
    "for i in np.arange(1,11):\n",
    "    if i == 1:\n",
    "        df = streets[0:int(totalsize * .1)]\n",
    "    else:\n",
    "        df = streets[((i-1)/10 * totalsize).astype(int): (((i)/10) * totalsize).astype(int)]\n",
    "        \n",
    "    sample = []\n",
    "    boxdata.append(df['tickperspot'])\n",
    "    for j in np.arange(1,1000):\n",
    "        sample.append(df['tickperspot'].sample(n = 20).median())\n",
    "        \n",
    "    sample = np.array(sample)\n",
    "        \n",
    "    means[i] = sample.mean()\n",
    "    stds[i] = sample.std()\n",
    "    normals = norm(loc = means[i], scale = stds[i])\n",
    "    \n",
    "    x = np.linspace(normals.ppf(0.01), \n",
    "                        normals.ppf(0.99), \n",
    "                        100)\n",
    "    labelstr = str(i * 10) + '%'\n",
    "    plt.plot(x, normals.pdf(x), label = labelstr, color =  plt.cm.RdYlGn(1-(i/10)))\n",
    "plt.legend( loc = 0)\n",
    "plt.title('Frequency curves of sampled street populations sorted and split by OLS fitted values ')\n",
    "plt.xlabel('Tickets per 100 Parking spots per year')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.boxplot(boxdata, showfliers = False)\n",
    "plt.title('Boxplot of Tickets per 100 Spots for population groups from OLS Model')\n",
    ";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to include some interactions, we'll look through all combinations of unbias predictors and add them to a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnlist = ['vvol_carea', 'vvol_trkea', 'vvol_busea', 'speed_ea', 'parkpermile', 'distance', 'oneway']\n",
    "formulastring = 'tickperspot ~ '\n",
    "\n",
    "formulastring += '+'.join(columns) \n",
    "\n",
    "for combo in itertools.combinations(columnlist, 2):\n",
    "    formulastring += '+' + combo[0] + '*' + combo[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS.from_formula(formulastring , streets)\n",
    "res = model.fit()\n",
    "plt.rc('figure', figsize=(12, 7))\n",
    "plt.text(0.01, 0.05, str(res.summary()), {'fontsize': 10}, fontproperties = 'monospace') \n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main interaction that has both a significant weight and p-value is parkpermile* distance, which gives our density a third dimension. We'll include that in our final model, along with the logarithmic predictors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formstring = 'tickperspot ~np.log(vvol_trkea)+np.log(vvol_carea)+np.log(vvol_busea)+np.log(speed_ea) + np.log(parkpermile)'\\\n",
    "        ' + parkpermile:distance + oneway'\n",
    "model = sm.OLS.from_formula(formstring , streets)\n",
    "res = model.fit()\n",
    "plt.rc('figure', figsize=(12, 7))\n",
    "plt.text(0.01, 0.05, str(res.summary()), {'fontsize': 10}, fontproperties = 'monospace') \n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_confidence_interval(data, confidence):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), stats.sem(a)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-do our central limit exercise once again. We'll save some confidence intervals for later. \n",
    "streets['fitted'] =  res.fittedvalues\n",
    "streets.sort_values(by = 'fitted', inplace = True)\n",
    "plt.figure(figsize = (10, 10))\n",
    "means = {}\n",
    "stds = {}\n",
    "lci = {}\n",
    "uci = {}\n",
    "df = streets\n",
    "sample = []\n",
    "boxdata = []\n",
    "#create baseline curve\n",
    "for j in np.arange(1,1000):\n",
    "    sample.append(df['tickperspot'].sample(n = 20).median())\n",
    " \n",
    "\n",
    "\n",
    "sample = np.array(sample)\n",
    "\n",
    "means['base'], lci['base'], uci['base'] = mean_confidence_interval(sample, .9)\n",
    "stds['base'] = sample.std()\n",
    "normals = norm(loc = means['base'], scale = stds['base'])\n",
    "\n",
    "x = np.linspace(normals.ppf(0.01), \n",
    "                    normals.ppf(0.99), \n",
    "                    100)\n",
    "labelstr = str(i * 10) + '%'\n",
    "plt.plot(x, normals.pdf(x), label = 'Baseline', color = 'black', linestyle = '--')\n",
    "\n",
    "        \n",
    "for i in np.arange(1,11):\n",
    "    if i == 1:\n",
    "        df = streets[streets.fitted <=  np.percentile(streets.fitted, i*10)]\n",
    "    else:\n",
    "        df = streets[(streets.fitted <=  np.percentile(streets.fitted, i*10)) & (streets.fitted >  np.percentile(streets.fitted, (i-1)*10))]\n",
    "               \n",
    "    sample = []\n",
    "    boxdata.append(df['tickperspot'])\n",
    "    for j in np.arange(1,1000):\n",
    "        sample.append(df['tickperspot'].sample(n = 20).median())\n",
    "        \n",
    "    sample = np.array(sample)\n",
    "        \n",
    "    means[i], lci[i], uci[i] = mean_confidence_interval(sample, .9)\n",
    "    stds[i] = sample.std()\n",
    "    normals = norm(loc = means[i], scale = stds[i])\n",
    "    \n",
    "    x = np.linspace(normals.ppf(0.01), \n",
    "                        normals.ppf(0.99), \n",
    "                        100)\n",
    "    labelstr = str(i * 10) + '%'\n",
    "    plt.plot(x, normals.pdf(x), label = labelstr, color =  plt.cm.RdYlGn(1-(i/10)))\n",
    "plt.legend( loc = 0)\n",
    "plt.title('Distribution curves of sampled street populations sorted and split by OLS fitted values ')\n",
    "plt.xlabel('Tickets per 100 Parking spots per year')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means[1] / means['base']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means[1] / means[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_worst = streets[streets.fitted >  np.percentile(streets.fitted, (90))]\n",
    "df_best = streets[streets.fitted <  np.percentile(streets.fitted, (10))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_worst.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The difference between the means of the most desired streets and the baseline are double in this model. This model was also better at identifying which streets would be the worst to park at. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "How long can I actually park at a residential overtime area, and hope I don't get a ticket?\n",
    "\n",
    "This one is much more theoretical and more just for a little challening fun, as we would need a lot more data to fully answer it with concrete numbers. We would need more information on travel paths of the officers, and potentially some data on the cars that were parked. However, we can try and take a stab at it using some high level numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets clarify the assumptions and process for estimating this probaility. There are two parts we must solve in this question. The first part is the probability that they come down the street you are parked in. This probability will increase with time. The second includes the probability they come back, and in what time frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1. We'll assume the same success rate of each street, as in every X parking spots he passes on any street, they'll get one. But how do we arrive at this? We'll need some very high level estimates. First, we'll estimate the total number of spots they pass. \n",
    "\n",
    "\n",
    "We'll assume he travels roughly 75% of the weighted average freeflow speed(while he is looking for offenders), as from visual estimates. We'll also assume their full day is about 75% utilized over 6 hours a day.We'll also take time out for writing tickets, ~ 2 minutes each. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streetdata = result_query('Select lineid, distance, park_supply, speed_ea from street_volume_data')\n",
    "\n",
    "ticket_data = result_query(\"Select * from ticket_data where ViolationDesc = 'RES/OT' \")\n",
    "\n",
    "address_data = result_query('Select address, lineid from address_data')\n",
    "\n",
    "df = ticket_data.merge(address_data, left_on = 'address', right_on = 'address')\n",
    "df = df.merge(streetdata, left_on = 'lineid', right_on = 'lineid')\n",
    "\n",
    "c.execute(\"Select Count(distinct lineid) from ticket_data t1 join address_data t2 on t1.address = t2.address\")\n",
    "totalticks = c.fetchone()[0]\n",
    "c.execute(\"Select Count(distinct lineid) from ticket_data t1 join address_data t2 on t1.address = t2.address where violationdesc = 'RES/OT'\")\n",
    "resticks = c.fetchone()[0]\n",
    "\n",
    "percent_res = resticks/totalticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the average of non street cleaning per day \n",
    "df['TickDate']= df['TickIssueDate'].apply(lambda x: pd.to_datetime(x).date())\n",
    "tix_by_officer = df.groupby(by = ['TickDate','TickBadgeIssued'], as_index = False)['TicketNumber'].size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see how many officers per day are patrolling\n",
    "officer_by_day = tix_by_officer.groupby(by = ['TickDate'], as_index = False).size().reset_index(name='counts')\n",
    "avg_officers = officer_by_day['counts'].mean()\n",
    "avg_officers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_tix = tix_by_officer['counts'].mean()\n",
    "avg_tix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly 22 officers and 18 tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "average_freeflow_speed = df['speed_ea'].mean() * .50\n",
    "\n",
    "validstreet = streetdata[streetdata.park_supply > 0 ]\n",
    "\n",
    "average_spots_per_mile = validstreet['park_supply'] / validstreet['distance']\n",
    "average_spots_per_mile = average_spots_per_mile.mean()\n",
    "print(average_spots_per_mile)                            \n",
    "total_spots_per_day = average_freeflow_speed * (6-avg_tix*2/60) * percent_res * average_spots_per_mile\n",
    "total_spots_per_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to find total residential parking spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now well take the average number of spots per street and multiply it by the total number of streets\n",
    "\n",
    "mean_parking_spots = df['park_supply'].mean()\n",
    "\n",
    "total_spots = mean_parking_spots * df['lineid'].nunique()\n",
    "total_spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_spots_checked = total_spots_per_day * avg_officers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_spots_checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arrival rate in miuntes\n",
    "average_checks = total_spots_checked / total_spots\n",
    "\n",
    "print(average_checks)\n",
    "arrival_rate = 10*60 / average_checks\n",
    "print(arrival_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can expect that each spot, on average, will be checked roughly 3.54 times a day.  Most residential permit areas are restricted from 8AM to 8PM, so 12 hours total, but take out 2 because you can't get a ticket after parking past the 10th hour. We'll use an exponential distribution  to plot the probability of seeing an officer in X time units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "x = np.linspace(0,400)\n",
    "ax = plt.figure()\n",
    "prob = stats.expon.cdf(x=x, scale= arrival_rate)\n",
    "plt.plot(x, prob, color = \"blue\", linewidth = 3)\n",
    "plt.title(\"Probability of a parking enforcement officer passing your car initially\")\n",
    "plt.xlim(0,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2. \n",
    "On each ticket, they write down what time they first came and marked your car, and then what time they wrote the ticket. This implies they have a list of which cars they marked and where, and attempt to go check on them. Unfortunately, the initial time checked and the time returned were not included in the data. We will have to look for special circumstances under which we can find a distribution. We will look for circumstances where a ticketing officer wrote two tickets on the same street, but not within roughly 2 and 4 hours  of each other. this is under the assumption he marked the first car after he wrote the second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TickDate'] = df['TickIssueDate'].apply(lambda x:  dt.datetime.strftime(pd.to_datetime(x),'%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['TickBadgeIssued', 'TickIssueDate', 'TicketNumber', 'TickIssueTime', 'lineid', 'TickDate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df, left_on = ['TickDate', 'lineid', 'TickBadgeIssued'], right_on = ['TickDate', 'lineid', 'TickBadgeIssued'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_minutes(x,y):\n",
    "    timedelta = y-x\n",
    "    \n",
    "    return timedelta.seconds / 60\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[(df.TicketNumber_x != df.TicketNumber_y) & (df.TickIssueTime_y > df.TickIssueTime_x)]\n",
    "df['delta'] = df.apply(lambda x: delta_minutes(pd.to_datetime(x['TickIssueDate_x']), pd.to_datetime(x['TickIssueDate_y'])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only include that between 1 and 4 hours\n",
    "df = df[(df.delta > 60) & (df.delta < 240)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will be the ditribution of the 'return rate'\n",
    "df['delta'].hist(bins = 'auto')\n",
    "plt.xlabel('Time from Initial Ticket')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting and something I would expect. It also implies for many of these streets that they will come intermediatey to check anyway, and it is most liekly a standard route. We only really care about the distribution after 120 minutes, when he can give you a ticket. We'll also filter on less than 3 hours, beuase the distribution tails off there, and I would expect if might be because they passed through multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.delta > 120) & (df.delta < 180)]\n",
    "df['delta'].hist(bins = 'auto')\n",
    "plt.xlabel('Time from Initial Ticket')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W'ell turn this into a cumulative distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bin_edges = np.histogram(df['delta'], bins = 'auto', normed = True)\n",
    "\n",
    "cdf = np.cumsum(counts)\n",
    "plt.plot(bin_edges[1:], cdf/cdf[-1])\n",
    "plt.xlabel('Time after initial marking')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('CDF of return probability after initial marking')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine this with our original probability of rate. Because these probabilities are sepaarate dimensions,we will have to use simulated data to create one probability distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add initial arrival time\n",
    "x = np.random.random(size = 1000)\n",
    "\n",
    "def f(x):\n",
    "    return -math.log(1.0 - x) / (1/arrival_rate)\n",
    "\n",
    "firstpass = [f(x) for x in x]\n",
    "#firstpass = firstpass.rvs(size = 1000)\n",
    "#Create discrete random variable from 2nd arrival rate distribution\n",
    "values = df['delta']\n",
    "probs = 1/ df['delta'].shape[0]\n",
    "combination = pd.DataFrame({'val': values, 'probs' : probs})\n",
    "df = combination.groupby(by = 'val', as_index = False)['probs'].sum()\n",
    "custom = stats.rv_discrete(values = (df['val'], df['probs']))\n",
    "secondpass = custom.rvs(size = 1000)\n",
    "\n",
    "totalprob =  firstpass + secondpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bin_edges = np.histogram(totalprob, bins = 'auto', normed = True)\n",
    "cdf = np.cumsum(counts)\n",
    "plt.plot(bin_edges[1:], cdf/cdf[-1])\n",
    "plt.title('CDF of Receiving Residential Overtime Ticket on Average SF Street')\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.xlabel(\"Time(minutes)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this study was based around the variability of arrival rates by streets, so lets use the arrival rates from our least and most patrolled streets, categorized from our regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that we can identify streets he is less likely to travel down, so lets compare some different probability plots. That of the highest category, that of the overall average, and that of the worst category. We'll also try to include some confidence intervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean(mean, title, color):\n",
    "    firstpass_mean = [f(x, mean) for x in x]\n",
    "    totalprob_mean =  firstpass_mean + secondpass\n",
    "    counts_mean, bin_edges_mean = np.histogram(totalprob_mean, bins = 'auto', normed = True)\n",
    "    cdf_mean = np.cumsum(counts_mean)    \n",
    "    plt.plot(bin_edges[1:], cdf/cdf[-1], color = color, label = title)\n",
    "    \n",
    "for i in range(1,11):\n",
    "    arrival = arrival_rate * means['base'] / means[i]\n",
    "    plot_mean(arrival, 'pop' + i, i / 10)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "\n",
    "arrival_lci = arrival_rate *   means['base'] / lci['base']\n",
    "arrival_uci = arrival_rate *  means['base'] / uci['base']\n",
    "   \n",
    "arrival_best = arrival_rate * means['base'] / means[1]\n",
    "arrival_best_lci = arrival_rate * means['base'] / lci[1]\n",
    "arrival_best_uci = arrival_rate * means['base'] / uci[1]\n",
    "\n",
    "arrival_worst = arrival_rate * means['base'] / means[10]\n",
    "arrival_worst_lci = arrival_rate * means['base'] / lci[10]\n",
    "arrival_worst_uci = arrival_rate * means['base'] / uci[10]\n",
    "\n",
    "x = np.random.random(size = 1000)\n",
    "\n",
    "def f(x, arrival):\n",
    "    return -math.log(1.0 - x) / (1/arrival)\n",
    "\n",
    "\n",
    "def plot_mean_ci(mean, lci, uci, title, color):\n",
    "    firstpass_lci = [f(x, lci) for x in x]\n",
    "    firstpass_mean = [f(x, mean) for x in x]\n",
    "    firstpass_uci = [f(x, mean) for x in x]\n",
    "    \n",
    "    totalprob_lci =  firstpass_lci + secondpass\n",
    "    totalprob_mean =  firstpass_mean + secondpass\n",
    "    totalprob_uci =  firstpass_uci + secondpass\n",
    "\n",
    "    counts_mean, bin_edges_mean = np.histogram(totalprob_mean, bins = 'auto', normed = True)\n",
    "    cdf_mean = np.cumsum(counts_mean)\n",
    "    \n",
    "    counts_low, bin_edges_low = np.histogram(totalprob_lci, bins = 'auto', normed = True)\n",
    "    cdf_low = np.cumsum(counts_low)\n",
    "\n",
    "    counts, bin_edges = np.histogram(totalprob_uci, bins = 'auto', normed = True)\n",
    "    cdf_high = np.cumsum(counts_high)\n",
    "    \n",
    "    plt.plot(bin_edges[1:], cdf/cdf[-1], color = color, label = title)\n",
    "    plt.fillbetween(range(mean.shape[0]), cdf_low, cdf_high, color_shading = color, alpha = .25)\n",
    "\n",
    "\n",
    "plot_mean(arrival, arrival_lci, arrival_uci, 'Baseline', blue)\n",
    "plot_mean(arrival_best, arrival_best_lci, arrival_best_uci, 'Best', green)\n",
    "plot_mean(arrival_worst, arrival_worst_lci, arrival_worst_uci, 'Baseline', red)\n",
    "plt.title('CDF of Receiving Residential Overtime Ticket on Average SF Street, with confidence intervals')\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.xlabel(\"Time(minutes)\")\n",
    "plt.legend()\n",
    "plt.xlim(120,600)\n",
    "plt.xticks(np.arange(120,720,30))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "\n",
    "arrival_lci = arrival_rate *   means['base'] / lci['base']\n",
    "arrival_uci = arrival_rate *  means['base'] / uci['base']\n",
    "   \n",
    "arrival_best = arrival_rate * means['base'] / means[1]\n",
    "arrival_best_lci = arrival_rate * means['base'] / lci[1]\n",
    "arrival_best_uci = arrival_rate * means['base'] / uci[1]\n",
    "\n",
    "arrival_worst = arrival_rate * means['base'] / means[10]\n",
    "arrival_worst_lci = arrival_rate * means['base'] / lci[10]\n",
    "arrival_worst_uci = arrival_rate * means['base'] / uci[10]\n",
    "\n",
    "x = np.random.random(size = 1000)\n",
    "\n",
    "def f(x, arrival):\n",
    "    return -math.log(1.0 - x) / (1/arrival)\n",
    "\n",
    "\n",
    "def plot_mean_ci(mean, lci, uci):\n",
    "    \n",
    "\n",
    "\n",
    "firstpass_best = [f(x, arrival_best) for x in x]\n",
    "\n",
    "firstpass_worst = [f(x, arrival_worst) for x in x]\n",
    "\n",
    "totalprob_best =  firstpass_best + secondpass\n",
    "\n",
    "totalprob_worst =  firstpass_worst + secondpass\n",
    "\n",
    "\n",
    "\n",
    "#Average\n",
    "\n",
    "counts, bin_edges = np.histogram(totalprob, bins = 'auto', normed = True)\n",
    "cdf = np.cumsum(counts)\n",
    "\n",
    "\n",
    "plt.plot(bin_edges[1:], cdf/cdf[-1], color = 'blue', label = 'baseline')\n",
    "plt.fillbetween(range(mean.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "counts, bin_edges = np.histogram(totalprob_best, bins = 'auto', normed = True)\n",
    "cdf = np.cumsum(counts)\n",
    "plt.plot(bin_edges[1:], cdf/cdf[-1], color = 'green', label = 'best category')\n",
    "\n",
    "\n",
    "\n",
    "counts, bin_edges = np.histogram(totalprob_worst, bins = 'auto', normed = True)\n",
    "cdf = np.cumsum(counts)\n",
    "plt.plot(bin_edges[1:], cdf/cdf[-1], color = 'red', label = 'worst category')\n",
    "\n",
    "\n",
    "\n",
    "plt.title('CDF of Receiving Residential Overtime Ticket on Average SF Street')\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.xlabel(\"Time(minutes)\")\n",
    "plt.legend()\n",
    "plt.xlim(120,600)\n",
    "plt.xticks(np.arange(120,720,30))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "Although more street cleaning tickets are given out at 8 AM, 12 PM is more effective per linear foot because it is in the middle of the day and people(myself) would forget. Also, less consistency (once a week, every other week, etc) increases the effectiveness of getting tickets (per street clean). 2 AM is least effective because no one dares try to wake up beforehand. \n",
    "We'll use the street cleaning ID on this one to look at what links get the most street cleaning tickets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep = result_query('Select t1.lineid, fromhour, tohour, weekday, totalpermonth,  distance, nhood, park_supply from street_sweep_data t1 '\n",
    "                  'join street_volume_data t2 on t1.lineid = t2.lineid')\n",
    "ticks = result_query(\"Select TicketNumber, TickIssueDate, TickIssueTime, lineid from ticket_data t1 join address_data t2 \"\n",
    "                     \" on t1.address = t2.address where ViolationDesc = 'STR CLEAN'\")\n",
    "weekdaydict = {0: 'Mon', 1:'Tues', 2:'Wed', 3:'Thu', 4:'Fri', 5:'Sat', 6:'Sun'}\n",
    "\n",
    "ticks['weekday'] = ticks['TickIssueDate'].apply(lambda x: weekdaydict[pd.to_datetime(x).weekday()])\n",
    "\n",
    "sweep.drop_duplicates(subset = ['lineid', 'weekday'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byclean = ticks.groupby(by = ['lineid', 'weekday'], as_index = False)['TicketNumber'].agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks = byclean.merge(sweep, left_on = ['lineid', 'weekday'], right_on = ['lineid', 'weekday'])\n",
    "ticks.dropna(subset= ['park_supply'], inplace = True)\n",
    "ticks = ticks[ticks.park_supply > 0]\n",
    "ticks['TicketNumber'] = ticks['TicketNumber'] / totalyears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks['miles_sweeped_year'] = ticks['totalpermonth'] * 12 * ticks['distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Success rate is basically tickets per mile swept\n",
    "ticks['success_rate'] = ticks['TicketNumber'] / ticks['miles_sweeped_year'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks.sort_values(by = 'success_rate', ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks as if our theory is holding up, with the highest success rates being those that are swept once per month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks.groupby(by = 'totalpermonth')['success_rate'].mean().plot(kind = 'bar')\n",
    "plt.title('Average Tickets per Mile swept number of sweeps per month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears out theory is true, proving that more inconsistently swept streets are more effective. Let's combine by StreetID so we can get streets that are MWF, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_street = ticks.groupby('lineid')[['totalpermonth', 'distance', 'TicketNumber']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_street['miles_sweeped_year'] = by_street['totalpermonth'] * 12 * by_street['distance']\n",
    "by_street['success_rate'] = by_street['TicketNumber'] / by_street['miles_sweeped_year'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_street.groupby('totalpermonth')['success_rate'].mean().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some streets are every single day of the week, which would show up as 7X5 =35 days here. In reality thats not possible. \n",
    "However, our theory holds that less frequent street cleaning is much more effective at getting tickets per mile swept, assuming their goal isn't actually related to street cleanliness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_hour = ticks.groupby(by = 'fromhour')['success_rate'].mean().plot(kind = 'bar')\n",
    "plt.title('Average Tickets per Mile Swept by Hour of Day Start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The other theory is incorrect, morning sweeping is the most effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "daydict = {'Mon':1, 'Tues':2, 'Wed' :3, 'Thu': 4, 'Fri': 5, 'Sat': 6, 'Sun': 7}\n",
    "df = ticks.groupby(by = 'weekday',as_index = False)['success_rate'].mean()\n",
    "df['daynum'] = df['weekday'].map(daydict)\n",
    "df.sort_values(by = 'daynum', inplace = True)\n",
    "df.drop(columns = 'daynum', inplace = True)\n",
    "plt.bar(x = df['weekday'],height = df['success_rate'])\n",
    "plt.title('Average Tickets per Mile swept by Day of Week')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weekends look to catch people off guard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
