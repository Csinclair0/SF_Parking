{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Database\n",
    "This jupyter notebook is fore creating the base db for the SF Parking EDA. There are 2 data sources we'll process in this book. \n",
    "1. Parking Ticket Data (csv)\n",
    "2. Address Data (csv)\n",
    "\n",
    "\n",
    "The goal is to process the raw data and create a relational database so we can do some more exploratory analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "raw_folder = '/home/colin/Desktop/SF_Parking/data/raw/'\n",
    "proc_folder = '/home/colin/Desktop/SF_Parking/data/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try:\n",
    "    os.rename('SF_Parking.db', 'SF_Parking_Backup.db')\n",
    "except: \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.remove('SF_Parking.db')\n",
    "except: \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "disk_engine = create_engine('sqlite:///SF_Parking.db')\n",
    "conn = sqlite3.connect('SF_Parking.db')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Ticket Data\n",
    "Here we'll simply load all the csv data into a raw ticket database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to return queries into DF\n",
    "def result_query(querystring):\n",
    "    resultdf = pd.read_sql(sql= querystring, con = conn)\n",
    "    \n",
    "    return resultdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/colin/Desktop/SF_Parking/data/raw//ticket_data/PRR_P000731_082418_070118_to_083118.csv file started at 2018-10-30 17:41:48.394829\n",
      "/home/colin/Desktop/SF_Parking/data/raw//ticket_data/PRR_P000731_082418_070118_to_083118.csv file finished in 0.32 minutes \n",
      "/home/colin/Desktop/SF_Parking/data/raw//ticket_data/PRR_P000731_082418_070116_to_123116.csv file started at 2018-10-30 17:42:07.512372\n",
      "/home/colin/Desktop/SF_Parking/data/raw//ticket_data/PRR_P000731_082418_070116_to_123116.csv file finished in 1.25 minutes \n",
      "/home/colin/Desktop/SF_Parking/data/raw//ticket_data/PRR_P000731_082418_010118_to_0630018.csv file started at 2018-10-30 17:43:03.828393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/colin/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/colin/Desktop/SF_Parking/data/raw//ticket_data/PRR_P000731_082418_010118_to_0630018.csv file finished in 2.37 minutes \n",
      "/home/colin/Desktop/SF_Parking/data/raw//ticket_data/PRR_P000731_082418_010117_to_0630017.csv file started at 2018-10-30 17:44:10.910549\n",
      "/home/colin/Desktop/SF_Parking/data/raw//ticket_data/PRR_P000731_082418_010117_to_0630017.csv file finished in 3.98 minutes \n",
      "/home/colin/Desktop/SF_Parking/data/raw//ticket_data/PRR_P000731_082418_070117_to_1231017.csv file started at 2018-10-30 17:45:48.015887\n",
      "/home/colin/Desktop/SF_Parking/data/raw//ticket_data/PRR_P000731_082418_070117_to_1231017.csv file finished in 6.52 minutes \n"
     ]
    }
   ],
   "source": [
    "#loop through all files and create dataframe, insert into db\n",
    "#c.execute('Drop table raw_ticket_data')\n",
    "start = dt.datetime.now()\n",
    "for csv_file in glob.glob(raw_folder + \"/ticket_data/PRR_*\"):\n",
    "    index_start = 1\n",
    "    j = 0\n",
    "    print('{} file started at {}'.format(csv_file, dt.datetime.now()))    \n",
    "    df = pd.read_csv(csv_file, encoding = 'utf-8', parse_dates = ['Tick Issue Date'])       \n",
    "    df = df.rename(columns = {c: c.replace(' ', '') for c in df.columns})\n",
    "    try:\n",
    "        df.to_sql('raw_ticket_data', conn, if_exists='append')\n",
    "    except:\n",
    "        print('File read error')\n",
    "       \n",
    "    \n",
    "    print ('{} file finished in {:03.2f} minutes '.format(csv_file, (dt.datetime.now()-start).seconds / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Address Data\n",
    "There are quite a lot of addresses missing so we'll have to do some cleaning, and generate some data of our own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/colin/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2961: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "#use our SF BLock Limits table to create a dictionary of dataframes for each street name. \n",
    "#We can use this to bounce against when we have a combination that doesnt match the raw address records\n",
    "#We'll also create a dataframe of street intersections\n",
    "columns_first = ['BlockStart', 'StreetName', 'Suffix']\n",
    "columns_second = ['BlockEnd', 'Cross1', 'Cross2', 'numbers']\n",
    "valid_suffix = ['ST', 'WY', 'DR', 'AV', 'LN', 'WAY', 'TER', 'PL', 'AVE']\n",
    "streetnamedict = {}\n",
    "\n",
    "with open(raw_folder + 'SF_Block_Limits_Table.txt') as f:\n",
    "    lines = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "streetintersections = pd.DataFrame(columns = ['Street', 'Suffix', 'Isection', 'Number'])\n",
    "   \n",
    "streetnamedict = {}\n",
    "suffixnums = pd.DataFrame(columns = ['Suffix', 'Min', 'Max', 'Count'])\n",
    "suffixnumsdefault = pd.DataFrame(columns = ['Suffix', 'Min', 'Max', 'Count'])\n",
    "streetnamedict['NA'] = suffixnumsdefault\n",
    "for idx, line in enumerate(lines):\n",
    "    rowsplit = line.split()\n",
    "    if len(rowsplit) == 5 and rowsplit[3] in valid_suffix:\n",
    "        suffix = rowsplit[3]\n",
    "        streetname = rowsplit[2]\n",
    "        minnum = int(rowsplit[1])\n",
    "        maxnum = int(lines[idx+1].split()[1])\n",
    "        suffixnums = streetnamedict.get(streetname, suffixnumsdefault)\n",
    "        totalsuffix = pd.DataFrame(columns = ['Suffix', 'Min', 'Max', 'Count'])\n",
    "        #print(lines[idx+1])\n",
    "        \n",
    "        isections = str(lines[idx+1]).replace( '/', ' ').split()[3:5]\n",
    "        dfrec1 = [streetname, streetname]\n",
    "        dfrec2 = [suffix, suffix]\n",
    "        dfrec3 = [isections[0], isections[1]]\n",
    "        dfrec4 = [minnum, maxnum]\n",
    "        newrecords = pd.DataFrame({'Street': dfrec1, 'Suffix': dfrec2, 'Isection': dfrec3, 'Number': dfrec4 })\n",
    "        streetintersections = streetintersections.append(newrecords)\n",
    "        \n",
    "        if suffixnums.shape[0] > 0:\n",
    "            suffixnumsame = suffixnums[suffixnums['Suffix'] == suffix]\n",
    "            suffixnumother = suffixnums[suffixnums['Suffix'] != suffix]\n",
    "            if suffixnumsame.shape[0] == 0:               \n",
    "                suffixlist = [suffix, minnum, maxnum, 0]\n",
    "                suffixnumsame.loc[0] = [value for value in suffixlist]\n",
    "\n",
    "            elif suffixnumsame.shape[0] == 1:            \n",
    "                if suffixnumsame['Max'][0] < maxnum:\n",
    "                    suffixnumsame['Max'][0] = maxnum\n",
    "\n",
    "                if suffixnumsame['Min'][0] > minnum:\n",
    "                    suffixnumsame['Min'][0] = minnum\n",
    "\n",
    "                suffixnumsame['Count'][0] += 1\n",
    "\n",
    "            totalsuffix = suffixnumother.append(suffixnumsame)\n",
    "        \n",
    "        else:\n",
    "           \n",
    "            suffixlist = [suffix, minnum, maxnum, 0]           \n",
    "            totalsuffix.loc[0] = [value for value in suffixlist]\n",
    "\n",
    "        totalsuffix.reset_index()\n",
    "        streetnamedict[streetname] = totalsuffix\n",
    "streetintersections.drop_duplicates(subset = ['Street', 'Isection'], inplace = True)   \n",
    "sqldf = streetintersections[['Number', 'Street']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to map numbers with their full streetname \n",
    "replacements = ['[^0-9a-zA-Z\\s]', '^0+']\n",
    "streetnums = {'1':'ST', '2': 'ND', '3': 'RD', '4': 'TH', '5': 'TH', '6': 'TH', '7': 'TH', '8': 'TH', '9': 'TH', '0': 'TH'}\n",
    "def replace_street(street):\n",
    "    if isinstance(street, str):\n",
    "        for rep in replacements:\n",
    "            street = re.sub(rep, \"\", street)\n",
    "    \n",
    "    streetint = re.findall(r'\\d+', str(street))\n",
    "    if len(streetint) > 0 and int(streetint[0]) < 100:\n",
    "        street = int(streetint[0])\n",
    "        \n",
    "        if street < 10:\n",
    "            street = '0' + str(street) + str(streetnums[str(street)])\n",
    "        elif street < 14:\n",
    "            street = str(street) + 'TH'\n",
    "        else:\n",
    "            street = str(street) + str(streetnums[str(street)[-1]])\n",
    "\n",
    "        \n",
    "    return street\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/colin/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (5,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['lon', 'lat', 'number', 'street', 'unit', 'city', 'district', 'region',\n",
       "       'postcode', 'id', 'hash', 'unnamed: 11'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read our address csv file into a df\n",
    "addresses = pd.read_csv(raw_folder + '/san_francisco_addresses.csv')\n",
    "addresses.columns = map(str.lower, addresses.columns)\n",
    "addresses.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove non number characters, \n",
    "def return_num(strnum):\n",
    "    if strnum != strnum or strnum == ' ':\n",
    "        return -1\n",
    "    else:\n",
    "        strnum = re.sub('[^1-9]', '', str(strnum))\n",
    "        return int(strnum)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some streets have names like 'AVENUE A' we dont want to strip out, only remove the last suffix if in the list\n",
    "valid_suffix = ['ST', 'WY', 'DR', 'AV', 'LN', 'WAY', 'TER', 'PL', 'BLVD', 'AVE']\n",
    "def return_street(streetname):\n",
    "    if streetname == None:\n",
    "        return streetname\n",
    "    if streetname.split(\" \")[-1] in valid_suffix:\n",
    "        return \" \".join(str(streetname).split(\" \")[:-1])\n",
    "    \n",
    "    return streetname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keepcolumns = ['lon', 'lat', 'number', 'street']\n",
    "addresses = addresses[keepcolumns]\n",
    "#Get rid of apartment number\n",
    "addresses['number'] = addresses['number'].apply(lambda x: re.findall( '\\d+', x)[0]).astype(int)\n",
    "\n",
    "#combine into an address\n",
    "addresses['address'] = addresses.apply(lambda x: str(x['number']) + \" \" + str(x['street']), axis = 1)\n",
    "\n",
    "#return just streetname as its only columns, remove suffix\n",
    "addresses['streetname'] = addresses['street'].apply(return_street)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(207698, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addresses.drop_duplicates(subset = 'address', inplace = True)\n",
    "addresses['type'] = 'known'\n",
    "addresses.to_sql('raw_address_data', if_exists = 'replace', con = conn)\n",
    "addresses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(268998, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's look for all combinations that wont yield any matches \n",
    "df = result_query('Select distinct tickstreetno , tickstreetname , count(*) total_tickets from raw_ticket_data t1'\n",
    "                      ' left join raw_address_data t2 on t1.TickStreetNo = t2.number and t1.TickStreetName = t2.streetname '\n",
    "                      \" where t2.address is null group by tickstreetno, tickstreetname \")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign a blocknumber to both known and unknown addresses\n",
    "\n",
    "df['TickStreetNo'] = df['TickStreetNo'].apply(return_num)\n",
    "df['TickStreetName'] = df['TickStreetName'].apply(replace_street)\n",
    "df['TickStreetName'] = df['TickStreetName'].apply(return_street)\n",
    "df['blocknum'] = df['TickStreetNo'].apply(lambda x: math.ceil(x/100))\n",
    "df.drop_duplicates(inplace = True)\n",
    "\n",
    "df2 = addresses\n",
    "df2['blocknum'] = df2['number'].apply(lambda x: math.ceil(x/100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge on same streetname, blocknumber, and side\n",
    "newdf = df.merge(df2, how = 'left', left_on = ['TickStreetName', 'blocknum'], \\\n",
    "                 right_on = ['streetname', 'blocknum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newdf(185917, 7)\n"
     ]
    }
   ],
   "source": [
    "#filter out those we couldnt find and save \n",
    "unfound = newdf[pd.isnull(newdf.number)]\n",
    "unfound['type'] == \"unknown\"\n",
    "\n",
    "#only include those we did, take the closest records coordinates, note we only drop on 'street' (street + suffix)\n",
    "#and not streetname , so we'll keep both suffixes if valid\n",
    "newdf = newdf[pd.isnull(newdf.number) == False]\n",
    "newdf['delta'] = np.abs(newdf['number'] - newdf['TickStreetNo'])\n",
    "newdf.sort_values(by = 'delta', inplace = True)\n",
    "\n",
    "newdf.drop_duplicates(subset = ['TickStreetName', 'TickStreetNo'], keep = 'first', inplace = True)\n",
    "\n",
    "#rename our columns, create an address, and put into sql\n",
    "newdf = newdf[[ 'lon', 'lat', 'TickStreetNo', 'street', 'address','streetname' ]]\n",
    "newdf.columns = ['lon', 'lat', 'number', 'street', 'address','streetname' ]\n",
    "newdf['address'] = newdf['number'].map(str) + ' ' + newdf['street']\n",
    "newdf.drop_duplicates(inplace = True)\n",
    "newdf['type'] = 'similar'\n",
    "newdf.to_sql('raw_address_data', conn, if_exists = 'append')\n",
    "print('newdf' + str(newdf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrap out those we know are invalid\n",
    "unfound = unfound[unfound.TickStreetNo < 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sometimes they put an intersection instead of any coordinates, lets try to solve that\n",
    "#function to return address of record that only put intersection\n",
    "def return_intersections(streetname):\n",
    "    if streetname != None and isinstance(streetname, str):\n",
    "        if ' AND ' in streetname:\n",
    "            streetnames = streetname.split(' AND ')\n",
    "            df = streetintersections[(streetintersections.Street == streetnames[0]) \\\n",
    "                                            & (streetintersections.Isection == streetnames[1])]\n",
    "            if df.shape[0] > 0:\n",
    "                return str(int(df['Number'].iloc[0])) + ' ' + df['Street'].iloc[0] + ' ' + df['Suffix'].iloc[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/colin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Look for any addresses that still don't match, try and identify them as an intersection\n",
    "isection = unfound[['TickStreetNo','TickStreetName', 'total_tickets']]\n",
    "isection['address'] = isection['TickStreetName'].apply(return_intersections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isection(83, 6)\n",
      "unfound (44012, 4)\n"
     ]
    }
   ],
   "source": [
    "#generate adddress data for intersections\n",
    "unfound = isection[pd.isnull(isection.address) == True]\n",
    "isection = isection[pd.isnull(isection.address) == False]\n",
    "isection = isection.merge(addresses, left_on = 'address', right_on = 'address')\n",
    "isection = isection[['number', 'streetname', 'street', 'address', 'lat', 'lon']]\n",
    "print('isection' + str(isection.shape))\n",
    "print('unfound ' + str(unfound.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "isection.to_sql('raw_address_data', if_exists = 'append', con = conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if no valid records found in dataframe, return the closest \n",
    "def return_closest(minnum, maxnum, num):\n",
    "    return min(np.abs(minnum - num), np.abs(maxnum - num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use street dictionary of dataframes to look for suffix of any combination that is not found\n",
    "#use most popular if more than one\n",
    "def return_streetname_unknown( streetnum, streetname):\n",
    "    if streetnum != streetnum:\n",
    "        strnum = ''\n",
    "    else:\n",
    "        strnum = str(int(streetnum) ) \n",
    "    \n",
    "    streetname = replace_street(streetname)\n",
    "    availablesuffix = streetnamedict.get(streetname, ' ')\n",
    "    suffix = '' \n",
    "    if isinstance(availablesuffix , pd.DataFrame):\n",
    "        \n",
    "        if availablesuffix.shape[0] == 1:\n",
    "            suffix = str(availablesuffix['Suffix'][0])\n",
    "        elif availablesuffix.shape[0] > 1:\n",
    "            validsuffix = availablesuffix[(availablesuffix['Min'] < streetnum) & (availablesuffix['Max'] > streetnum)]\n",
    "            if validsuffix.shape[0] > 0:\n",
    "                validsuffix = validsuffix.sort_values(by = 'Count', ascending = False)\n",
    "            else:  \n",
    "                availablesuffix['delta'] = availablesuffix.apply(lambda x: return_closest(x['Min'], x['Max'], streetnum), axis = 1)\n",
    "                validsuffix = availablesuffix.sort_values(by = 'delta', ascending = True)\n",
    "                \n",
    "            validsuffix = validsuffix.reset_index()\n",
    "            \n",
    "            suffix = str(validsuffix['Suffix'][0])\n",
    "\n",
    "\n",
    "    if streetname == None:\n",
    "        streetname = ''\n",
    "    streetname = str(streetname)    \n",
    "    streetname += ' ' + suffix\n",
    "    return streetname \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfound.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321902"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfound['total_tickets'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/colin/.local/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6499: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We don't want to look up every missing address, it will take a while so lets check out a cdf of how many tickets we can get \n",
    "print(unfound.shape[0])\n",
    "plt.hist(unfound['total_tickets'], normed = 1, cumulative = True, bins = 1000)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [15:00<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "#Extremely steep, lets look up the top 500\n",
    "#Check to make sure the location returned is valid\n",
    "def check_location(location):\n",
    "    if location.latitude > 35 and location.latitude < 39 and location.longitude > -123 and location.longitude < -120:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#Attempt to reverge geocode using OpenStreetMap, \n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "geolocator = Nominatim(user_agent = 'SF_Parking_EDA')\n",
    "\n",
    "def create_locs(address):\n",
    "    try:\n",
    "        location = geolocator.geocode(address, timeout = 10)\n",
    "    except:\n",
    "        location = None\n",
    "    time.sleep(1)\n",
    "\n",
    "    if location != None and check_location(location):\n",
    "        return (location.latitude, location.longitude )   \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "#return an estimated address, lookup top 500\n",
    "tqdm.pandas()\n",
    "unfound['street'] = unfound.apply(lambda x: return_streetname_unknown(x['TickStreetNo'], x['TickStreetName']), axis = 1)\n",
    "#combine into an address\n",
    "unfound['address'] = unfound.apply(lambda x: str(x['TickStreetNo']) + \" \" + str(x['street']), axis = 1)\n",
    "lookup = unfound.sort_values(by = 'total_tickets', ascending = False)[:500]\n",
    "lookup['coordinates'] = lookup['address'].progress_apply(lambda x: create_locs(x + ' SAN FRANCISCO CA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TickStreetNo</th>\n",
       "      <th>TickStreetName</th>\n",
       "      <th>total_tickets</th>\n",
       "      <th>address</th>\n",
       "      <th>street</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>7020</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4716</th>\n",
       "      <td>-1</td>\n",
       "      <td>MISSION</td>\n",
       "      <td>6556</td>\n",
       "      <td>-1 MISSION ST</td>\n",
       "      <td>MISSION ST</td>\n",
       "      <td>(37.7932361, -122.3928913)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4436</th>\n",
       "      <td>-1</td>\n",
       "      <td>MARKET</td>\n",
       "      <td>6371</td>\n",
       "      <td>-1 MARKET ST</td>\n",
       "      <td>MARKET ST</td>\n",
       "      <td>(37.79407, -122.3949185)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7003</th>\n",
       "      <td>-1</td>\n",
       "      <td>VAN NESS</td>\n",
       "      <td>5562</td>\n",
       "      <td>-1 VAN NESS</td>\n",
       "      <td>VAN NESS</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2789</th>\n",
       "      <td>-1</td>\n",
       "      <td>GEARY</td>\n",
       "      <td>4340</td>\n",
       "      <td>-1 GEARY ST</td>\n",
       "      <td>GEARY ST</td>\n",
       "      <td>(37.787819, -122.4061608)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TickStreetNo TickStreetName  total_tickets        address      street  \\\n",
       "0               -1           None           7020           -1                 \n",
       "4716            -1        MISSION           6556  -1 MISSION ST  MISSION ST   \n",
       "4436            -1         MARKET           6371   -1 MARKET ST   MARKET ST   \n",
       "7003            -1       VAN NESS           5562   -1 VAN NESS    VAN NESS    \n",
       "2789            -1          GEARY           4340    -1 GEARY ST    GEARY ST   \n",
       "\n",
       "                     coordinates  \n",
       "0                           None  \n",
       "4716  (37.7932361, -122.3928913)  \n",
       "4436    (37.79407, -122.3949185)  \n",
       "7003                        None  \n",
       "2789   (37.787819, -122.4061608)  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn everything we looked up into a usable df, append to other data\n",
    "lookup.dropna(subset = ['coordinates'], inplace = True)\n",
    "lookup['lat'] = lookup['coordinates'].apply(lambda x: x[0])\n",
    "lookup['lon'] = lookup['coordinates'].apply(lambda x: x[1])\n",
    "lookup.rename(columns = {'TickStreetNo':'number', 'TickStreetName':'streetname'}, inplace = True)\n",
    "lookup = lookup[['lat', 'lon', 'street', 'number', 'streetname', 'address']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter on everything we couldn't find, we'll insert it without any coordinates anyway\n",
    "unfound = unfound[unfound['address'].isin(lookup['address']) == False]\n",
    "unfound['type'] = 'unfound'\n",
    "\n",
    "\n",
    "lookup['type'] = 'searched'\n",
    "lookup.to_sql('raw_address_data', if_exists = 'append', con = conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addreses(393909, 8)\n"
     ]
    }
   ],
   "source": [
    "addresses = result_query('Select * from raw_address_data')\n",
    "print('addreses' + str(addresses.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We're going to look up all addresses and associate a neighborhood with them, we'll use this later on\n",
    "from shapely.geometry import Point\n",
    "#create a point for each address\n",
    "addresses['geometry'] = addresses.apply(lambda x: Point(x['lon'], x['lat']), axis = 1)\n",
    "point = gpd.GeoDataFrame(addresses['geometry'])\n",
    "point.crs = {'init': 'epsg:4326'}\n",
    "\n",
    "#load neighborhoods file\n",
    "poly  = gpd.GeoDataFrame.from_file(raw_folder + 'AnalysisNeighborhoods.geojson')\n",
    "\n",
    "#join each point into a neighborhood\n",
    "from geopandas.tools import sjoin\n",
    "pointInPolys = sjoin(point, poly, how='left')\n",
    "\n",
    "#convert to string so we can merge back\n",
    "addresses['geometry'] = addresses['geometry'].astype(str)\n",
    "pointInPolys['geometry'] = pointInPolys['geometry'].astype(str)\n",
    "\n",
    "#drop duplicates and put in sql\n",
    "addresses = addresses.merge(pointInPolys, left_on = 'geometry', right_on = 'geometry')\n",
    "addresses.drop(columns = ['geometry', 'index', 'index_right'], inplace = True)\n",
    "addresses.drop_duplicates(subset = 'address', inplace = True)\n",
    "addresses['number'] = addresses['number'].astype(int)\n",
    "addresses.to_sql('address_data', conn, if_exists = 'replace')\n",
    "print('addreses' + str(addresses.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add all other addresses that didn't have coordinates\n",
    "unfound.rename(columns = {'TickStreetNo':'number', 'TickStreetName': 'streetname'}, inplace = True)\n",
    "unfound.drop(columns = 'total_tickets', inplace = True)\n",
    "unfound['number'] = unfound['number'].astype(int)\n",
    "unfound.to_sql('address_data', if_exists = 'append', con = conn)\n",
    "print('unfound' + str(unfound.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ticket data doesnt include the street suffix, so sometimes we may not know which street its located on. lets see some of our problem streets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we'll find out which only have one matching street suffix\n",
    "grouped = addresses.groupby(by = ['number', 'streetname'], as_index = False)['address'].agg('count')\n",
    "grouped.sort_values(by = 'address', ascending = False) \n",
    "grouped.columns = ['number', 'streetname', 'count_ad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We're going to split all single and double addresses. This will help when looking up addresses later\n",
    "single_address = grouped[grouped.count_ad ==1]\n",
    "single_address = single_address.merge(addresses, left_on = ['number', 'streetname'], right_on = ['number', 'streetname'])\n",
    "single_address.to_sql('single_address', conn, if_exists = 'replace')\n",
    "double_address = addresses[addresses.address.isin(single_address['address']) == False]\n",
    "\n",
    "print(single_address.shape)\n",
    "print(double_address.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Raw Ticket Data\n",
    "\n",
    "We're going to take our raw data and put it in usable form. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any address that could have two locations we're going to have to look up other tickets that were given by that officer in that time period, and then sort our best choice based on proximity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli(p):\n",
    "    if np.random.random() < p:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "nhoodtype = result_query('Select nhood, violationdesc, count(*) tickets from raw_ticket_data t1 join single_address t2 '\n",
    "                         ' on t1.TickStreetNo = t2.number and t1.TickStreetName = t2.streetname group by nhood, violationdesc')\n",
    "\n",
    "def return_time(time):\n",
    "    if time == None:\n",
    "        time = [0,0]\n",
    "    else:\n",
    "        time = time.split(\":\")\n",
    "    if len(time) < 2:\n",
    "        time = [0,0]\n",
    "    return dt.timedelta(hours = int(time[0]), minutes = int(time[1]))\n",
    "def return_address(row):\n",
    "    #For anything we can find using our merges, we use a ratio of total tickets for that violation description and neighborhood\n",
    "    streetnum = row['TickStreetNo']\n",
    "    streetname = row['TickStreetName']\n",
    "    ticket_type = row['ViolationDesc']\n",
    "    df = double_address[(double_address.number == streetnum) & (double_address.streetname == streetname)]  \n",
    "    if df.shape[0] > 1:\n",
    "        if len(re.findall('\\d+', streetname)) > 0:\n",
    "\n",
    "            if ticket_type == 'RES/OT' and int(re.findall('\\d+', streetname)[0]) > 15 and (streetnum < 2200 or streetnum > 2600):\n",
    "                df_st = df[df.street.str.contains(\"ST\")]\n",
    "                if df_st.shape[0] == 1:\n",
    "                    return str(int(streetnum)) + \" \" + df_st['street'].iloc[0]\n",
    "\n",
    "            if ticket_type == 'RES/OT' and int(re.findall('\\d+', streetname)[0]) > 21:\n",
    "                df_st = df[df.street.str.contains(\"ST\")]\n",
    "                if df_st.shape[0] == 1:\n",
    "                    return str(int(streetnum)) + \" \" + df_st['street'].iloc[0]\n",
    "            \n",
    "        df['ViolationDesc'] = ticket_type\n",
    "        \n",
    "        df_2 = df.merge(nhoodtype, left_on = ['nhood', 'ViolationDesc'], right_on = ['nhood', 'ViolationDesc'])\n",
    "        \n",
    "        if df_2.shape[0] > 0:\n",
    "            totalcounts = df_2['tickets'].sum()\n",
    "            topcount = df_2['tickets'].iloc[0]\n",
    "            topchoice = bernoulli(float(topcount / totalcounts))   \n",
    "            return str(int(streetnum)) + \" \" + df_2['street'].iloc[topchoice]\n",
    "\n",
    "        totalcounts = addresses[addresses.streetname == streetname].shape[0]\n",
    "        topcount = addresses[addresses.streetname == streetname]['street'].value_counts().iloc[0]\n",
    "        topchoice = bernoulli(float(topcount / totalcounts))   \n",
    "        return str(int(streetnum)) + \" \" + df['street'].iloc[topchoice]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add time to date\n",
    "def Time(row):\n",
    "    try:\n",
    "        timeadd = dt.datetime.strptime(row['TickIssueTime'], '%H:%M').time()\n",
    "    except:\n",
    "        timeadd = dt.datetime.strptime('00:00', '%H:%M').time()\n",
    "        \n",
    "    newtime = dt.datetime.combine(dt.datetime.strptime(row['TickIssueDate'], '%Y-%m-%d %H:%M:%S') , timeadd)\n",
    "    return newtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = conn.cursor()\n",
    "c.execute('Select Count(*) from raw_ticket_data')\n",
    "totalleft = c.fetchone()[0]\n",
    "print('{} total rows required'.format(totalleft))\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to loop through all raw ticket records, process them in chunks, and insert them into a processed table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = result_query('Select Ticketnumber, TickIssueDate, TickIssueTime, ViolationDesc, '\n",
    "                  ' VehMake, TickRPPlate, TickStreetNo, TickMeter, Agency, TickBadgeIssued, '\n",
    "                   'TickStreetName , TotalPaid, TotalAmtDue from raw_ticket_data ')\n",
    "columnlist = df_total.columns.tolist()\n",
    "df_total.sort_values(by = 'TickIssueDate', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_time_delta(time):\n",
    "    if time == None:\n",
    "        time = [0,0]\n",
    "    else:\n",
    "        time = time.split(\":\")\n",
    "    if len(time) < 2:\n",
    "        time = [0,0]\n",
    "    return dt.timedelta(hours = int(time[0]), minutes = int(time[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_cost(coststring):\n",
    "    coststring = re.sub('[^1-9]', '', str(coststring))\n",
    "    try:\n",
    "        intreturn = int(coststring)\n",
    "    except:\n",
    "        intreturn = 0\n",
    "        \n",
    "    return intreturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a chunk to process, process main columns, and create a timedelta column for the ticket time\n",
    "df = df_total[0:10000]\n",
    "df['TickStreetNo'] = df['TickStreetNo'].apply(return_num)\n",
    "df['ViolationDesc'] = df['ViolationDesc'].apply(lambda x: x.replace('METER DTN','MTR OUT DT'))\n",
    "df['TickStreetName'] = df['TickStreetName'].apply(replace_street)\n",
    "df['TotalPaid'] = df['TotalPaid'].apply(lambda x: re.sub('[^1-9]', '', str(x)))\n",
    "df['TotalAmtDue'] = df['TotalAmtDue'].apply(lambda x: re.sub('[^1-9]', '', str(x)))\n",
    "df['TickRPPlate'] = df['TickRPPlate'].apply(lambda x: 'None' if len(re.findall('[\\w+]', str(x))) == 0 else str(x).replace('[^\\w+]', ''))\n",
    "df['Tdelt'] = df['TickIssueTime'].apply(return_time_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split out what we know and what we'll have to run a merge on \n",
    "df_1 = df.merge(single_address, how = 'left',  left_on = ['TickStreetNo', 'TickStreetName'], right_on = ['number', 'streetname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[pd.isnull(df_1.number)][['TickStreetName', 'TickStreetNo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only include that which will return an address   \n",
    "df_2 = df.merge(double_address, left_on = ['TickStreetNo', 'TickStreetName'], right_on = ['number', 'streetname']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge on badge and neighborhood, sort by time, take top record, anything we cant merge on we'll pass to our function   \n",
    "df_2 = df_2.merge(df_1, how = 'left', left_on = ['TickIssueDate', 'TickBadgeIssued', 'nhood'], right_on = ['TickIssueDate', 'TickBadgeIssued', 'nhood'])\n",
    "df_3 = df_2[pd.isnull(df_2['Tdelt_y'])]\n",
    "df_2.dropna(subset = ['Tdelt_y'], inplace = True)\n",
    "df_2['timedelta'] = df_2.apply(lambda x: np.abs(x['Tdelt_y'] - x['Tdelt_x']), axis = 1)\n",
    "df_2.sort_values(by = 'timedelta', inplace = True)\n",
    "\n",
    "#Replace the columns t\n",
    "df_2.columns = [col.replace('_x', '') for col in df_2.columns]\n",
    "df_3.columns = [col.replace('_x', '') for col in df_3.columns]\n",
    "df_2.drop_duplicates(subset = 'TicketNumber', inplace = True)\n",
    "df_3['address'] = df_3.progress_apply(return_address, axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "#append both single and merged records\n",
    "df = df_1.append(df_2)\n",
    "df['TickIssueDate'] = df.apply(Time, axis = 1)\n",
    "df = df[columnlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets loop through entire dataframe and do all of them\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "n = 500000  #chunk row size\n",
    "totalsize = df_total.shape[0]\n",
    "indexes = [i for i in range(0,totalsize, n)]\n",
    "columnlist = df_total.columns.tolist()\n",
    "columnlist.append('address')\n",
    "tqdm.pandas()\n",
    "j = 1\n",
    "for i in indexes:\n",
    "    df = df_total[i:i+n]\n",
    "    print('Iteration {} started at {}. {} records left'.format(j, dt.datetime.now(), totalsize))   \n",
    "    df['TickStreetNo'] = df['TickStreetNo'].apply(return_num)\n",
    "    df['ViolationDesc'] = df['ViolationDesc'].apply(lambda x: x.replace('METER DTN','MTR OUT DT'))\n",
    "    df['TickStreetName'] = df['TickStreetName'].apply(replace_street)\n",
    "    df['TickStreetName'] = df['TickStreetName'].apply(return_street)\n",
    "    df['TotalPaid'] = df['TotalPaid'].apply(return_cost)\n",
    "    df['TotalAmtDue'] = df['TotalAmtDue'].apply(lambda x: re.sub('[^1-9]', '', str(x)))\n",
    "    df['TickRPPlate'] = df['TickRPPlate'].apply(lambda x: 'None' if len(re.findall('[\\w+]', str(x))) == 0 else str(x).replace('[^\\w+]', ''))\n",
    "    df['Tdelt'] = df['TickIssueTime'].apply(return_time_delta)\n",
    "    \n",
    "    \n",
    "    #Split out what we know and what we'll have to run a function on\n",
    "    df_1 = df.merge(single_address, left_on = ['TickStreetNo', 'TickStreetName'], right_on = ['number', 'streetname'])\n",
    "    df_2 = df.merge(double_address, left_on = ['TickStreetNo', 'TickStreetName'], right_on = ['number', 'streetname'])  \n",
    "\n",
    "    #Merge on badge and neighborhood, sort by time, take top record, anyhting we cant merge on we'll pas to our function   \n",
    "    df_2 = df_2.merge(df_1, how = 'left', left_on = ['TickIssueDate', 'TickBadgeIssued', 'nhood'], right_on = ['TickIssueDate', 'TickBadgeIssued', 'nhood'])\n",
    "    df_3 = df_2[pd.isnull(df_2['Tdelt_y'])]\n",
    "    df_2.dropna(subset = ['Tdelt_y'], inplace = True)\n",
    "    df_2['timedelta'] = df_2.apply(lambda x: np.abs(x['Tdelt_y'] - x['Tdelt_x']), axis = 1)\n",
    "    df_2.sort_values(by = 'timedelta', inplace = True)\n",
    "    \n",
    "    #Replace the columns t\n",
    "    df_2.columns = [col.replace('_x', '') for col in df_2.columns]\n",
    "    df_3.columns = [col.replace('_x', '') for col in df_3.columns]\n",
    "    df_2.drop_duplicates(subset = 'TicketNumber', inplace = True)\n",
    "    df_3['address'] = df_3.progress_apply(return_address, axis = 1)\n",
    "    \n",
    "    df = df_1.append(df_2)\n",
    "    df = df.append(df_3)\n",
    "    df['TickIssueDate'] = df.apply(Time, axis = 1)\n",
    "    df = df[columnlist]    \n",
    "                                                                                \n",
    "    if i == 0:\n",
    "        df.to_sql('ticket_data', if_exists = 'replace',con = conn)\n",
    "    else:\n",
    "        df.to_sql('ticket_data', if_exists = 'append',con = conn)\n",
    "        \n",
    "    totalsize -= n\n",
    "    j+=1\n",
    "    \n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
